# DE-GAN-for-IC-Dataset


Document enhancement aims to produce high-quality images from low-quality images. It removes imperfections, ensuring readability and suitability for Opti- cal Character Recognition (OCR) in subsequent processing stages of our pipeline. Various factors contribute to recognizing document data, such as blurry frames during video capture and physical flaws like smudges, presenting multiple chal- lenges for this task.
Recent advancements in deep learning have led to the development of inno- vative techniques to tackle this challenge. Establishing the Document Imaging and Binarization Contest (DIBCO) by the International Conference on Docu- ment Analysis and Recognition( ICDAR) in 2009 standardized dataset formats for document analysis, prompting numerous approaches with varying degrees of success. Notably, document enhancement and binarisation using iterative deep learning (DeepOtsu)[2] introduces an iterative deep learning neural network incorporating traditional binarization in its final layer, enabling it to address pixel-level degradation rather than solely focusing on pixel-level label matching. This model is trained to identify and rectify degraded sections within document images. Similarly, Image deraining [17] utilizes a conditional-GAN[8] to clean images degraded by rain. Additionally, in DocEnTr: An End-to-End Document Image Enhancement Transformer[14], the images will be split into patches before feeding to the encoder module, where tokens are generated and matched to each latent representation. The Decoder then aims to obtain the token information and generate the target pixel values for each patch.
Our approach integrates state-of-the-art techniques to enhance document images captured within video streams. At the heart of our methodology lies the DE-GAN framework, a novel instantiation of conditional GANs tailored for image-to-image translation tasks through the strategic interplay between its generator and discriminator components, as shown in Figure 3. DE-GAN iteratively refines degraded document images, transitioning them towards higher clarity and fidelity.
The architecture of DE-GAN incorporates skip connections to facilitate in- formation flow across layers, ensuring the preservation of critical document details. The model utilizes a generator structured on the U-net architecture[11] for facilitating image-to-image translation. Skip connections are incorporated to alleviate information loss. A discriminator, a fully convolutional network (FCN)[7], distinguishes real from generated images, and adversarial training improves image enhancement. Experiments on document de-blurring, binariza- tion, and watermark removal demonstrate superior performance over traditional methods, validated by competitive benchmark scores. Additionally, batch nor- malization layers enhance training stability and convergence speed, enabling efficient learning of document enhancement features. [2]
Our methodology has demonstrated promising results through extensive ex- perimentation and validation in elevating the clarity and fidelity of document images captured within video frames. In addition to enhancing document clar- ity captured in the video frames, our methodology facilitates enhancing doc- uments for OCR (Optical Character Recognition). While the original docu- ments captured in video frames often present challenges for OCR systems due to degradation and noise, our enhanced documents exhibit improved legibility and structure, enabling more accurate and reliable text extraction.


The training methodology employed for DE-GAN followed a meticulously struc- tured approach aimed at optimizing its performance and efficacy. The docu- ments detected in the video frame are cropped and transformed to a PDF per- spective. Initially, patches of size 256 × 256 are extracted from these degraded document images, serving as the input data for the generator. Subsequently, these patches are utilized with ground truth and degraded patches to train the discriminator. The primary objective of the discriminator was to differentiate between authentic and generated images, thereby compelling the generator to produce outputs that closely resemble authentic images.
This adversarial training paradigm is visually shown in Figure 7, illustrating the interplay between the generator and discriminator.We employ the Adam optimizer with a 2×10−4 learning rate to facilitate the training process, ensuring efficient convergence and model optimization.
For training and validation purposes, 1500 images for training and 300 im- ages for testing are utilized from the IC dataset. The training dataset extracted
9
 
 Figure 6: Experiment Retreival
overlapped patches, each measuring 256 × 256 pixels. This approach allowed for comprehensive dataset coverage, facilitating robust learning (Figure7) and generalization capabilities within the model.
(a) MSE for 50 Epochs (b) PSNR for 50 Epochs (c) SSIM for 50 Epochs
Figure 7: DE-GAN-Training Metrics
To demonstrate the effectiveness of DE-GAN in enhancing degraded docu- ment images, we provide an illustrative example in Figure 8. These examples provide compelling evidence of DE-GAN’s proficiency in enhancing documents to a state resembling the ground truth. Through qualitative assessment, we showcase the model’s ability to improve degraded document images’ visual qual- ity and fidelity, thereby highlighting its practical utility in real-world scenarios.
